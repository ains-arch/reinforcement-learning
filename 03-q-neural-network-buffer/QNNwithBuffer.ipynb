{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "id": "KJUp6QBRY-Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7a193c-ea81-418f-fc20-7d443b09b426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import gymnasium as gym\n",
        "from gymnasium.envs.registration import register\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "K_GLg7lbayhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Give colab access to your google drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "7PWN1PkGe66q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2c4a41-3a49-49bd-a22c-d43ec4c14cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change current directory to folder with MiniPacMan\n",
        "%cd /gdrive/MyDrive/reinforcement_learning/QNNwBuffer"
      ],
      "metadata": {
        "id": "1SCX1d90YjOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104ecdd7-35c5-47fe-fb66-bab43016faf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/reinforcement_learning/QNNwBuffer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import MiniPacMan environment class definition\n",
        "from MiniPacManGym import MiniPacManEnv"
      ],
      "metadata": {
        "id": "GCa5TYdVWL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Register MiniPacMan in your gymnasium environments\n",
        "register(\n",
        "    id=\"MiniPacMan-v0\",\n",
        "    entry_point=MiniPacManEnv,  # Update with your actual module path\n",
        "    max_episode_steps=20          # You can also set a default here\n",
        ")"
      ],
      "metadata": {
        "id": "TcY1Q97RRy6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c734121-883e-4a94-fbaf-4c006e57c521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment MiniPacMan-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a MiniPacMan gymnasium environment\n",
        "env = gym.make(\"MiniPacMan-v0\", render_mode=\"human\", frozen_ghost=False)"
      ],
      "metadata": {
        "id": "k7hwnC7Ob9VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(6 * 6, 128)  # Flattened board (6x6 → 36)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 4)  # Output 4 possible actions (Up, Down, Left, Right)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = state.view(state.shape[0], -1)  # Flatten from (batch_size, 6, 6) → (batch_size, 36)\n",
        "        x = self.activation(self.fc1(state))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation here; raw Q-values\n",
        "        return x  # Output shape: (batch_size, 4)"
      ],
      "metadata": {
        "id": "Y6irumLQsc1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
        "        return torch.stack(states), actions, torch.tensor(rewards), torch.stack(next_states), torch.tensor(dones)"
      ],
      "metadata": {
        "id": "8D31lBLpRUC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = QNetwork() #initialize a Q network\n",
        "Q_optimizer = torch.optim.Adam(Q.parameters(), lr=0.01) #feel free to change this"
      ],
      "metadata": {
        "id": "6-TgasGh9Q11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set hyperparams -- play with any of these!\n",
        "gamma=0.999\n",
        "buffer_size=1000\n",
        "batch_size=32\n",
        "num_episodes=10000\n",
        "\n",
        "RB=ReplayBuffer(buffer_size) #initialize Replay Buffer\n",
        "epsilon=1 #initialize epsilon\n",
        "\n",
        "for e in range(num_episodes):\n",
        "  new_obs,info=env.reset()\n",
        "  new_obs=torch.tensor(new_obs,dtype=torch.float32)\n",
        "\n",
        "  done=False\n",
        "  truncated=False\n",
        "  steps=0\n",
        "\n",
        "  while not done and not truncated: #Loop for one episode\n",
        "    obs=new_obs\n",
        "\n",
        "    #choose action\n",
        "    t=np.random.random()\n",
        "    if t>epsilon:\n",
        "      action = torch.argmax(Q(new_obs.unsqueeze(0))).item()\n",
        "    else:\n",
        "      action=torch.randint(4,(1,)).item()\n",
        "\n",
        "    #take a step:\n",
        "    new_obs,reward, done, truncated, info=env.step(action)\n",
        "    new_obs=torch.tensor(new_obs,dtype=torch.float32)\n",
        "    RB.push(obs,action,reward,new_obs,done)\n",
        "    steps+=1\n",
        "\n",
        "    if len(RB.buffer)>=batch_size:\n",
        "      states, actions, rewards, next_states, dones=RB.sample(batch_size)\n",
        "      actions = torch.tensor(actions, dtype=torch.long)\n",
        "      rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "      dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "      preds = Q(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "      targets = rewards + (1 - dones) * gamma * torch.max(Q(next_states))\n",
        "      loss = ((targets - preds) ** 2).mean()\n",
        "      Q_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      Q_optimizer.step()\n",
        "\n",
        "  #reduce episilon if its not too low:\n",
        "  epsilon=[epsilon-0.00015 if epsilon > 0.01 else 0.01][0]\n",
        "\n",
        "  #periodic reporting:\n",
        "  if e>0 and e%100==0:\n",
        "    print(f'episode: {e}, steps: {steps}, epislon: {epsilon},win: {reward==10}')\n",
        "\n"
      ],
      "metadata": {
        "id": "0fe-YvvwKpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20dc66d-3ce6-4482-f09d-20ce89f14945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-ea10ae6c041f>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rewards = torch.tensor(rewards, dtype=torch.float32)\n",
            "<ipython-input-61-ea10ae6c041f>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dones = torch.tensor(dones, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 100, steps: 5, epislon: 0.9848500000000017,win: False\n",
            "episode: 200, steps: 1, epislon: 0.9698500000000033,win: False\n",
            "episode: 300, steps: 1, epislon: 0.954850000000005,win: False\n",
            "episode: 400, steps: 2, epislon: 0.9398500000000066,win: False\n",
            "episode: 500, steps: 2, epislon: 0.9248500000000083,win: False\n",
            "episode: 600, steps: 2, epislon: 0.9098500000000099,win: False\n",
            "episode: 700, steps: 1, epislon: 0.8948500000000116,win: False\n",
            "episode: 800, steps: 3, epislon: 0.8798500000000132,win: False\n",
            "episode: 900, steps: 3, epislon: 0.8648500000000149,win: False\n",
            "episode: 1000, steps: 3, epislon: 0.8498500000000165,win: False\n",
            "episode: 1100, steps: 3, epislon: 0.8348500000000182,win: False\n",
            "episode: 1200, steps: 1, epislon: 0.8198500000000198,win: False\n",
            "episode: 1300, steps: 1, epislon: 0.8048500000000215,win: False\n",
            "episode: 1400, steps: 1, epislon: 0.7898500000000231,win: False\n",
            "episode: 1500, steps: 4, epislon: 0.7748500000000248,win: False\n",
            "episode: 1600, steps: 4, epislon: 0.7598500000000264,win: False\n",
            "episode: 1700, steps: 5, epislon: 0.7448500000000281,win: False\n",
            "episode: 1800, steps: 6, epislon: 0.7298500000000298,win: False\n",
            "episode: 1900, steps: 6, epislon: 0.7148500000000314,win: False\n",
            "episode: 2000, steps: 1, epislon: 0.6998500000000331,win: False\n",
            "episode: 2100, steps: 1, epislon: 0.6848500000000347,win: False\n",
            "episode: 2200, steps: 10, epislon: 0.6698500000000364,win: False\n",
            "episode: 2300, steps: 1, epislon: 0.654850000000038,win: False\n",
            "episode: 2400, steps: 8, epislon: 0.6398500000000397,win: False\n",
            "episode: 2500, steps: 1, epislon: 0.6248500000000413,win: False\n",
            "episode: 2600, steps: 2, epislon: 0.609850000000043,win: False\n",
            "episode: 2700, steps: 6, epislon: 0.5948500000000446,win: False\n",
            "episode: 2800, steps: 6, epislon: 0.5798500000000463,win: True\n",
            "episode: 2900, steps: 8, epislon: 0.5648500000000479,win: False\n",
            "episode: 3000, steps: 1, epislon: 0.5498500000000496,win: False\n",
            "episode: 3100, steps: 2, epislon: 0.5348500000000512,win: False\n",
            "episode: 3200, steps: 15, epislon: 0.5198500000000529,win: False\n",
            "episode: 3300, steps: 1, epislon: 0.5048500000000545,win: False\n",
            "episode: 3400, steps: 3, epislon: 0.4898500000000562,win: False\n",
            "episode: 3500, steps: 4, epislon: 0.47485000000005784,win: False\n",
            "episode: 3600, steps: 1, epislon: 0.4598500000000595,win: False\n",
            "episode: 3700, steps: 4, epislon: 0.44485000000006114,win: False\n",
            "episode: 3800, steps: 10, epislon: 0.4298500000000628,win: False\n",
            "episode: 3900, steps: 3, epislon: 0.41485000000006444,win: False\n",
            "episode: 4000, steps: 3, epislon: 0.3998500000000661,win: False\n",
            "episode: 4100, steps: 1, epislon: 0.38485000000006775,win: False\n",
            "episode: 4200, steps: 8, epislon: 0.3698500000000694,win: True\n",
            "episode: 4300, steps: 6, epislon: 0.35485000000007105,win: False\n",
            "episode: 4400, steps: 8, epislon: 0.3398500000000727,win: False\n",
            "episode: 4500, steps: 4, epislon: 0.32485000000007436,win: False\n",
            "episode: 4600, steps: 13, epislon: 0.309850000000076,win: False\n",
            "episode: 4700, steps: 6, epislon: 0.29485000000007766,win: True\n",
            "episode: 4800, steps: 1, epislon: 0.2798500000000793,win: False\n",
            "episode: 4900, steps: 17, epislon: 0.26485000000008097,win: False\n",
            "episode: 5000, steps: 1, epislon: 0.2498500000000826,win: False\n",
            "episode: 5100, steps: 2, epislon: 0.23485000000008147,win: False\n",
            "episode: 5200, steps: 6, epislon: 0.21985000000008034,win: True\n",
            "episode: 5300, steps: 10, epislon: 0.20485000000007922,win: True\n",
            "episode: 5400, steps: 6, epislon: 0.1898500000000781,win: True\n",
            "episode: 5500, steps: 5, epislon: 0.17485000000007697,win: False\n",
            "episode: 5600, steps: 8, epislon: 0.15985000000007585,win: True\n",
            "episode: 5700, steps: 2, epislon: 0.14485000000007472,win: False\n",
            "episode: 5800, steps: 3, epislon: 0.1298500000000736,win: False\n",
            "episode: 5900, steps: 6, epislon: 0.11485000000007342,win: True\n",
            "episode: 6000, steps: 6, epislon: 0.09985000000007369,win: True\n",
            "episode: 6100, steps: 6, epislon: 0.08485000000007395,win: True\n",
            "episode: 6200, steps: 6, epislon: 0.06985000000007421,win: True\n",
            "episode: 6300, steps: 8, epislon: 0.05485000000007448,win: True\n",
            "episode: 6400, steps: 6, epislon: 0.03985000000007474,win: False\n",
            "episode: 6500, steps: 6, epislon: 0.024850000000074857,win: True\n",
            "episode: 6600, steps: 8, epislon: 0.009850000000074775,win: True\n",
            "episode: 6700, steps: 10, epislon: 0.01,win: True\n",
            "episode: 6800, steps: 8, epislon: 0.01,win: True\n",
            "episode: 6900, steps: 20, epislon: 0.01,win: True\n",
            "episode: 7000, steps: 20, epislon: 0.01,win: False\n",
            "episode: 7100, steps: 6, epislon: 0.01,win: True\n",
            "episode: 7200, steps: 6, epislon: 0.01,win: True\n",
            "episode: 7300, steps: 6, epislon: 0.01,win: True\n",
            "episode: 7400, steps: 14, epislon: 0.01,win: True\n",
            "episode: 7500, steps: 10, epislon: 0.01,win: True\n",
            "episode: 7600, steps: 8, epislon: 0.01,win: True\n",
            "episode: 7700, steps: 8, epislon: 0.01,win: True\n",
            "episode: 7800, steps: 6, epislon: 0.01,win: True\n",
            "episode: 7900, steps: 14, epislon: 0.01,win: True\n",
            "episode: 8000, steps: 4, epislon: 0.01,win: False\n",
            "episode: 8100, steps: 14, epislon: 0.01,win: True\n",
            "episode: 8200, steps: 6, epislon: 0.01,win: True\n",
            "episode: 8300, steps: 14, epislon: 0.01,win: True\n",
            "episode: 8400, steps: 8, epislon: 0.01,win: True\n",
            "episode: 8500, steps: 16, epislon: 0.01,win: True\n",
            "episode: 8600, steps: 12, epislon: 0.01,win: True\n",
            "episode: 8700, steps: 6, epislon: 0.01,win: True\n",
            "episode: 8800, steps: 10, epislon: 0.01,win: True\n",
            "episode: 8900, steps: 6, epislon: 0.01,win: True\n",
            "episode: 9000, steps: 8, epislon: 0.01,win: True\n",
            "episode: 9100, steps: 10, epislon: 0.01,win: True\n",
            "episode: 9200, steps: 6, epislon: 0.01,win: True\n",
            "episode: 9300, steps: 10, epislon: 0.01,win: True\n",
            "episode: 9400, steps: 6, epislon: 0.01,win: True\n",
            "episode: 9500, steps: 8, epislon: 0.01,win: True\n",
            "episode: 9600, steps: 14, epislon: 0.01,win: True\n",
            "episode: 9700, steps: 20, epislon: 0.01,win: True\n",
            "episode: 9800, steps: 6, epislon: 0.01,win: True\n",
            "episode: 9900, steps: 14, epislon: 0.01,win: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()\n",
        "done = False\n",
        "truncated = False\n",
        "\n",
        "while not done and not truncated:\n",
        "    env.render()\n",
        "    obs=torch.tensor(obs,dtype=torch.float32)\n",
        "    action=torch.argmax(Q(obs.unsqueeze(0))).item()\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    sleep(1)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "env.render()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "0SXyI97eNx6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a43e5fa-31a4-4099-bb9a-73fdbea1adf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxx\n",
            "x····x\n",
            "x····x\n",
            "x··ᗣ·x\n",
            "x···ᗧx\n",
            "xxxxxx\n",
            "\n"
          ]
        }
      ]
    }
  ]
}