{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "id": "KJUp6QBRY-Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39e1498-e73e-474e-f61f-2b928787a9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import gymnasium as gym\n",
        "from gymnasium.envs.registration import register\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "K_GLg7lbayhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Give colab access to your google drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "7PWN1PkGe66q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408f8af5-a588-4a56-ac8c-205f854a76b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change current directory to folder with MiniPacMan\n",
        "%cd /gdrive/MyDrive/reinforcement_learning/DQNwPacman"
      ],
      "metadata": {
        "id": "1SCX1d90YjOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54169ad6-6a05-413a-a78c-f14e45953d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/reinforcement_learning/DQNwPacman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import MiniPacMan environment class definition\n",
        "from MiniPacManGymv2 import MiniPacManEnv"
      ],
      "metadata": {
        "id": "GCa5TYdVWL2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Register MiniPacMan in your gymnasium environments\n",
        "register(\n",
        "    id=\"MiniPacMan-v2\",\n",
        "    entry_point=MiniPacManEnv,  # Update with your actual module path\n",
        "    max_episode_steps=20          # You can also set a default here\n",
        ")"
      ],
      "metadata": {
        "id": "TcY1Q97RRy6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a MiniPacMan gymnasium environment\n",
        "env = gym.make(\"MiniPacMan-v2\", render_mode=\"human\", frozen_ghost=False)"
      ],
      "metadata": {
        "id": "k7hwnC7Ob9VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(6 * 6, 128)  # Flattened board (6x6 → 36)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 4)  # Output 4 possible actions (Up, Down, Left, Right)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = state.view(state.shape[0], -1)  # Flatten from (batch_size, 6, 6) → (batch_size, 36)\n",
        "        x = self.activation(self.fc1(state))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation here; raw Q-values\n",
        "        return x  # Output shape: (batch_size, 4)"
      ],
      "metadata": {
        "id": "Y6irumLQsc1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
        "        return torch.stack(states), actions, torch.tensor(rewards), torch.stack(next_states), torch.tensor(dones)"
      ],
      "metadata": {
        "id": "8D31lBLpRUC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set hyperparams -- play with any of these!\n",
        "gamma=0.9\n",
        "buffer_size=2000\n",
        "batch_size=32\n",
        "num_episodes=5000\n",
        "\n",
        "Q_target = QNetwork() # initialize target Q network\n",
        "Q_policy = QNetwork() # initialize policy Q newtwork\n",
        "Q_optimizer = torch.optim.Adam(Q_policy.parameters(), lr=0.0005)\n",
        "\n",
        "Q_target.load_state_dict(Q_policy.state_dict()) #copy weights from policy to target\n",
        "\n",
        "RB=ReplayBuffer(buffer_size) #initialize Replay Buffer\n",
        "epsilon=1 #initialize epsilon\n",
        "\n",
        "for e in range(num_episodes):\n",
        "  new_obs,info=env.reset()\n",
        "  new_obs=torch.tensor(new_obs,dtype=torch.float32)\n",
        "\n",
        "  done=False\n",
        "  truncated=False\n",
        "  steps=0\n",
        "\n",
        "  while not done and not truncated: #Loop for one episode\n",
        "    obs=new_obs\n",
        "\n",
        "    #choose action\n",
        "    t=np.random.random()\n",
        "    if t>epsilon:\n",
        "      action = torch.argmax(Q_policy(new_obs.unsqueeze(0))).item()\n",
        "    else:\n",
        "      action=torch.randint(4,(1,)).item()\n",
        "\n",
        "    #take a step:\n",
        "    new_obs,reward, done, truncated, info=env.step(action)\n",
        "    new_obs=torch.tensor(new_obs,dtype=torch.float32)\n",
        "    RB.push(obs,action,reward,new_obs,done)\n",
        "    steps+=1\n",
        "\n",
        "    if len(RB.buffer)>=batch_size:\n",
        "      states, actions, rewards, next_states, dones=RB.sample(batch_size)\n",
        "      actions = torch.tensor(actions, dtype=torch.long)\n",
        "      rewards = rewards.clone().detach() if isinstance(rewards, torch.Tensor) else torch.tensor(rewards, dtype=torch.float32)\n",
        "      dones = dones.clone().detach() if isinstance(dones, torch.Tensor) else torch.tensor(dones, dtype=torch.float32)\n",
        "      preds = Q_policy(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "      targets = rewards + (1 - dones.float()) * gamma * Q_target(next_states).max(1)[0].detach()\n",
        "      loss = ((targets - preds) ** 2).mean()\n",
        "      Q_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      Q_optimizer.step()\n",
        "\n",
        "  #if reward==20:\n",
        "    #print(\"got the ghost!\")\n",
        "  if e%100==0:\n",
        "    Q_target.load_state_dict(Q_policy.state_dict())\n",
        "\n",
        "  #reduce episilon if its not too low\n",
        "  epsilon = max(0.01, epsilon * 0.999)\n",
        "\n",
        "  #periodic reporting:\n",
        "  if e>0 and e%100==0:\n",
        "    print(f'episode: {e}, steps: {steps}, epislon: {epsilon},win: {reward==20}')"
      ],
      "metadata": {
        "id": "0fe-YvvwKpAZ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b0386d-ac4e-4ba0-8b93-469ec1e44296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 100, steps: 1, epislon: 0.9038873549665959,win: False\n",
            "episode: 200, steps: 1, epislon: 0.8178301806491574,win: False\n",
            "episode: 300, steps: 1, epislon: 0.7399663251239436,win: False\n",
            "episode: 400, steps: 10, epislon: 0.6695157201007336,win: False\n",
            "episode: 500, steps: 7, epislon: 0.6057725659163237,win: False\n",
            "episode: 600, steps: 4, epislon: 0.548098260578011,win: False\n",
            "episode: 700, steps: 2, epislon: 0.4959150020176678,win: False\n",
            "episode: 800, steps: 10, epislon: 0.44869999946146477,win: False\n",
            "episode: 900, steps: 14, epislon: 0.4059802359226587,win: False\n",
            "episode: 1000, steps: 6, epislon: 0.36732772934619257,win: False\n",
            "episode: 1100, steps: 7, epislon: 0.33235524492954527,win: False\n",
            "episode: 1200, steps: 1, epislon: 0.3007124156643058,win: False\n",
            "episode: 1300, steps: 20, epislon: 0.2720822322326576,win: False\n",
            "episode: 1400, steps: 1, epislon: 0.2461778670932771,win: False\n",
            "episode: 1500, steps: 20, epislon: 0.22273980093919937,win: False\n",
            "episode: 1600, steps: 1, epislon: 0.2015332227394583,win: False\n",
            "episode: 1700, steps: 4, epislon: 0.18234567731717977,win: False\n",
            "episode: 1800, steps: 4, epislon: 0.1649849368967147,win: False\n",
            "episode: 1900, steps: 1, epislon: 0.14927707529619813,win: False\n",
            "episode: 2000, steps: 13, epislon: 0.13506472547210188,win: True\n",
            "episode: 2100, steps: 18, epislon: 0.12220550295922675,win: True\n",
            "episode: 2200, steps: 12, epislon: 0.11057057941158951,win: False\n",
            "episode: 2300, steps: 7, epislon: 0.10004339195341891,win: False\n",
            "episode: 2400, steps: 12, epislon: 0.09051847541007228,win: False\n",
            "episode: 2500, steps: 8, epislon: 0.08190040571973876,win: False\n",
            "episode: 2600, steps: 3, epislon: 0.07410284394064628,win: False\n",
            "episode: 2700, steps: 10, epislon: 0.06704767127628951,win: True\n",
            "episode: 2800, steps: 6, epislon: 0.060664206453048174,win: False\n",
            "episode: 2900, steps: 10, epislon: 0.05488849760960279,win: True\n",
            "episode: 3000, steps: 13, epislon: 0.049662681604038215,win: True\n",
            "episode: 3100, steps: 1, epislon: 0.04493440431994225,win: False\n",
            "episode: 3200, steps: 13, epislon: 0.04065629616391608,win: True\n",
            "episode: 3300, steps: 11, epislon: 0.03678549749984046,win: True\n",
            "episode: 3400, steps: 10, epislon: 0.03328322926552661,win: True\n",
            "episode: 3500, steps: 3, epislon: 0.030114404470033673,win: False\n",
            "episode: 3600, steps: 11, epislon: 0.027247276679492435,win: True\n",
            "episode: 3700, steps: 10, epislon: 0.024653121969839265,win: True\n",
            "episode: 3800, steps: 12, epislon: 0.022305951160147018,win: True\n",
            "episode: 3900, steps: 11, epislon: 0.02018224944360293,win: True\n",
            "episode: 4000, steps: 15, epislon: 0.018260740807661956,win: True\n",
            "episode: 4100, steps: 20, epislon: 0.016522174883251375,win: False\n",
            "episode: 4200, steps: 10, epislon: 0.014949134087605212,win: True\n",
            "episode: 4300, steps: 13, epislon: 0.01352585912861506,win: True\n",
            "episode: 4400, steps: 10, epislon: 0.012238091122537187,win: True\n",
            "episode: 4500, steps: 10, epislon: 0.011072928743333644,win: True\n",
            "episode: 4600, steps: 10, epislon: 0.010018698972517958,win: True\n",
            "episode: 4700, steps: 13, epislon: 0.01,win: True\n",
            "episode: 4800, steps: 11, epislon: 0.01,win: True\n",
            "episode: 4900, steps: 11, epislon: 0.01,win: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space)\n",
        "new_obs, info = env.reset()\n",
        "print(f\"Reset observation: {new_obs}, Expected space: {env.observation_space}\")\n",
        "new_obs, reward, done, truncated, info = env.step(action)\n",
        "print(f\"Step observation: {new_obs}, Expected space: {env.observation_space}\")\n"
      ],
      "metadata": {
        "id": "aKjV_7Hm2ceg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()\n",
        "done = False\n",
        "truncated = False\n",
        "\n",
        "while not done and not truncated:\n",
        "    env.render()\n",
        "    obs=torch.tensor(obs,dtype=torch.float32)\n",
        "    action=torch.argmax(Q_policy(obs.unsqueeze(0))).item()\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    sleep(1)\n",
        "    clear_output(wait=True)\n",
        "\n",
        "env.render()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "0SXyI97eNx6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d8a92b-6944-4b53-8a35-ad26db0d84f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xxxxxx\n",
            "x····x\n",
            "x····x\n",
            "xᗧ···x\n",
            "x····x\n",
            "xxxxxx\n",
            "\n"
          ]
        }
      ]
    }
  ]
}